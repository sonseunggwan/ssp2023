{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df169d8-4c37-4450-97b5-6cc781a2ae4a",
   "metadata": {},
   "source": [
    "# DEEE725 Speech Signal Processing Lab\n",
    "### 2023 Spring, Kyungpook National University \n",
    "### Instructor: Gil-Jin Jang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09645527-0887-405f-9a9a-05da9cb45270",
   "metadata": {},
   "source": [
    "# Project 1 Isolated digit recognition in noisy environments\n",
    "\n",
    "- Assigned: 2023/04/21\n",
    "- Due: 2023/05/04\n",
    "- Required dataset: \n",
    "    1. [training data](lab05.pdf)\n",
    "    1. [validation data](lab05.md)\n",
    "    1. [test data](lab05.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8c951-5371-4e28-af69-2854658a82e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "# import packages, define analysis parameters and draw parameters, audio file preparation, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9515d0d3-8af0-47b1-bba5-aedbf3a1f4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary pacakages\n",
    "# strange issue: keep the import order to prevent matplotlib error\n",
    "#  import matplotlib -> librosa -> pyplot -> librosa.display\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import librosa\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "#from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "# display wav files\n",
    "import IPython #라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc26acb-6267-4773-8f15-ca4abb077ff4",
   "metadata": {},
   "source": [
    "오디오 파일들의 경로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827c344e-09c7-46fe-b841-0d3ad0f05dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add '/' if path is not a null string : 경로 파일 만드는거\n",
    "def addpath(path, file):\n",
    "    if len(path) == 0: \n",
    "        return file\n",
    "    else:\n",
    "        return path + '/' + file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a593f2-a698-4eb8-b876-9d53cda83ebe",
   "metadata": {},
   "source": [
    "신호 분석과 스펙트럼을 그리기 위한 다음의 parameter 들을 정의한다.\n",
    "입력 파일의 sampling frequency 를 이용하여 shift size 를 sample 수로 정의하기 위해 사용된다.\n",
    "- `Ts`: shift length in seconds, default 0.01 sec = 10 ms. \n",
    "- `Tf`: frame length in seconds, default 0.02 sec = 20 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94df199-6bc7-4a5f-9c4e-50ee25d7aebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for signal analysis\n",
    "# Fs = 16000  native sampling frequency (wav file 에 정의된 것) 을 사용하면 필요 없음\n",
    "Ts = 0.01   # 10 ms shift size\n",
    "Tf = 0.02   # 20 ms frame size 타임 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa43cec4-4631-40db-9f72-1d2827a21db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "spectrum 을 그리기 위한 parameters.\n",
    "- `cmap_plot`: colormap. default value is `pyplot.cm.bone_r` (최소값 흰색, 최대값 검은색 의 gray scale) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d45a30-c37a-4ddc-9a0a-8f3f3275e202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameters for drawing\n",
    "#cmap_plot = plt.cm.bone # default colormap for spectrogram, gray\n",
    "cmap_plot = plt.cm.bone_r # default colormap for spectrogram, gray, reversed\n",
    "#cmap_plot = plt.cm.plasma \n",
    "#cmap_plot = plt.cm.inferno\n",
    "#FIG_SIZE = (15,10)   # obsolete\n",
    "FIG_SIZE = (8,3) #Figure 사이즈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b3a5e5-0083-47ae-9916-715c6466fb2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### 이전 lab 들에서 정의한 함수들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0a2f81-d319-4137-99ee-2dde50c70ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# draw spectrogram\n",
    "from gjdrawspectrogram3 import drawspectrogram3\n",
    "\n",
    "# linear phase FIR filter design from magnitudes of the frequency components\n",
    "from gjfiroverlapadd import getLPHFIRFFT\n",
    "\n",
    "# trapezoidal overlap add for FIR filtering\n",
    "from gjfiroverlapadd import firoverlapadd\n",
    "\n",
    "# save audio in wav format\n",
    "import gjwavfile as wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbe1916-81e9-4942-bd0b-d105a74bf6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### load speech and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699ce62b-6c74-4d57-9cf9-73ee1dca1ae7",
   "metadata": {},
   "source": [
    "오디오 파일이 16 kHz, mono 인지 확인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631adcd2-1699-4f7c-b75a-2c4a75d4c40d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1\n",
      "10 2\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros(10)\n",
    "print(len(x), x.ndim)\n",
    "x = np.zeros((10,2))\n",
    "print(len(x), x.ndim) #zero로 채워주는 배열 생성 (쓰지는 않는다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24dd206d-9074-49a1-8373-3a0f9dfd892a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_audio_file(file, defFs, checkMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    if defFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(defFs, Fs, file))\n",
    "        return False\n",
    "    elif checkMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            return False\n",
    "        return True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def convert_audio_file(file, forceFs, forceMono):\n",
    "    signal, Fs = librosa.load(file, sr=None, mono=False)\n",
    "    changed = False\n",
    "    if forceFs != Fs:\n",
    "        print('sampling rate mismatch, %d != %d for file %s'%(forceFs, Fs, file))\n",
    "        signal, Fs = librosa.load(file, sr=forceFs, mono=False)\n",
    "        changed = True\n",
    "    elif forceMono == True:\n",
    "        if signal.ndim != 1:\n",
    "            print('not mono file %s, shape='%(file), signal.shape)\n",
    "            signal, Fs = librosa.load(file, sr=forceFs, mono=True)\n",
    "            changed = True\n",
    "    elif size(signal) <= 0:\n",
    "        print('wrong audio file %s, shape='%(file), signal.shape)\n",
    "        return False\n",
    "    if changed == True:\n",
    "        wav.writewav(file, Fs, signal, maxval=1.0)\n",
    "        print('updating', file)\n",
    "    return changed #오디오파일이 제대로 된건지, 이상한 부분은 없는지 check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f7c5bf3-1193-4279-b2fe-5815ac37b37e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InkooJeon: false 0 / 100\n",
      "\n",
      "Dandyst: false 0 / 100\n",
      "\n",
      "11jeonghy: false 0 / 100\n",
      "\n",
      "son: false 0 / 100\n",
      "\n",
      "YouYeNa: false 0 / 100\n",
      "\n",
      "shin3875: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainroot = 'segmented-train'\n",
    "'''\n",
    "labels_all = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'YouYeNa',\n",
    "                'chlee',\n",
    "                'deokkyukwon',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "                'ohjihyeon',\n",
    "                'son',\n",
    "               }\n",
    "''' \n",
    "labels_train = {'11jeonghy', \n",
    "                'Dandyst', \n",
    "                'InkooJeon',\n",
    "                'shin3875',\n",
    "                'YouYeNa',\n",
    "                'son',\n",
    "               } #\n",
    "\n",
    "# check\n",
    "Fs = 16000 \n",
    "for subname in labels_train:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(trainroot, addpath(subname, basename)) #특정 파일을 가져오는 명령어\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1 #파일이 맞는지 check\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342c0678-3f89-45cb-af59-b6f088ef6058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chlee: false 0 / 100\n",
      "\n",
      "kyeong: false 0 / 100\n",
      "\n",
      "do: false 0 / 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valroot = 'segmented-val'\n",
    "valclean = addpath(valroot, 'org')\n",
    "labels_val = {\n",
    "                'chlee',\n",
    "                'do',\n",
    "                'kyeong',\n",
    "               }\n",
    "\n",
    "# check\n",
    "Fs = 16000\n",
    "for subname in labels_val:\n",
    "    num_files = 0\n",
    "    num_false_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            file = addpath(valclean, addpath(subname, basename))\n",
    "            num_files += 1\n",
    "            if check_audio_file(file, Fs, True) == False:\n",
    "                num_false_files += 1\n",
    "    print('%s: false %d / %d\\n'%(subname, num_false_files, num_files)) #Validation 파일도 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073a311-4b6c-44c0-aa1c-4d9d666ad3ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### HMM training and test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b52602-9088-4556-9b0e-259ede899922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartProbPrior=\n",
      "[1. 0. 0.]\n",
      "TransMatPrior=\n",
      "[[0.5 0.5 0. ]\n",
      " [0.  0.5 0.5]\n",
      " [0.  0.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "############################################################################################## \n",
    "# extract MFCC features\n",
    "def extmfcc(file):\n",
    "    samplerate, d = wavfile.read(file)\n",
    "    #features.append(mfcc(d, nwin=int(samplerate * 0.03), fs=samplerate, nceps= 6)[0])\n",
    "    x = np.float32(d)\n",
    "    hop=samplerate//100\n",
    "    mc = mfcc(y=x, sr=samplerate, n_mfcc=num_mfcc, hop_length=hop, win_length=hop*2)\n",
    "    return np.transpose(mc, (1,0))\n",
    "\n",
    "def initByBakis(inumstates, ibakisLevel):\n",
    "    startprobPrior = np.zeros(inumstates)\n",
    "    startprobPrior[0: ibakisLevel - 1] = 1/float((ibakisLevel - 1))\n",
    "    transmatPrior = getTransmatPrior(inumstates, ibakisLevel)\n",
    "    return startprobPrior, transmatPrior\n",
    "\n",
    "def getTransmatPrior(inumstates, ibakisLevel):\n",
    "    transmatPrior = (1 / float(ibakisLevel)) * np.eye(inumstates)\n",
    "\n",
    "    for i in range(inumstates - (ibakisLevel - 1)):\n",
    "        for j in range(ibakisLevel - 1):\n",
    "            transmatPrior[i, i + j + 1] = 1. / ibakisLevel\n",
    "\n",
    "    for i in range(inumstates - ibakisLevel + 1, inumstates):\n",
    "        for j in range(inumstates - i - j):\n",
    "            transmatPrior[i, i + j] = 1. / (inumstates - i)\n",
    "\n",
    "    return transmatPrior\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# hyperparameters - CHANGE THEM TO IMPROVE PERFORMANCE\n",
    "# 1. number of MFCC (feature dimension)\n",
    "num_mfcc = 6\n",
    "#num_mfcc = 10\n",
    "#num_mfcc = 13\n",
    "# 2. Parameters needed to train GMMHMM\n",
    "m_num_of_HMMStates = 3  # number of states\n",
    "m_num_of_mixtures = 2  # number of mixtures for each hidden state\n",
    "m_covarianceType = 'diag'  # covariance type\n",
    "m_n_iter = 10  # number of iterations\n",
    "m_bakisLevel = 2\n",
    "m_startprobPrior, m_transmatPrior = initByBakis(m_num_of_HMMStates,m_bakisLevel)\n",
    "print(\"StartProbPrior=\"); print(m_startprobPrior)\n",
    "print(\"TransMatPrior=\"); print(m_transmatPrior)\n",
    "\n",
    "\n",
    "############################################################################################## \n",
    "# acoustic model definition\n",
    "class SpeechModel:\n",
    "    def __init__(self,Class,label):\n",
    "        self.traindata = np.zeros((0,num_mfcc))\n",
    "        self.Class = Class\n",
    "        self.label = label\n",
    "        self.model  = hmm.GMMHMM(n_components = m_num_of_HMMStates, n_mix = m_num_of_mixtures, \\\n",
    "                transmat_prior = m_transmatPrior, startprob_prior = m_startprobPrior, \\\n",
    "                covariance_type = m_covarianceType, n_iter = m_n_iter)\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def train_digits(rootpath, speakers, tag, num_trials=10):    \n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_trainingsetfeatures = []\n",
    "    m_trainingsetlabels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_trainingsetfeatures.append(mc)\n",
    "                m_trainingsetlabels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    ntrain = len(m_trainingsetlabels)\n",
    "\n",
    "    print(\"[training] number of labels and features = %d, %d\" % \n",
    "            ( len(m_trainingsetlabels), len(m_trainingsetfeatures)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # model initialization\n",
    "    gmmhmmindexdict = {}\n",
    "    index = 0\n",
    "    for word in spoken:\n",
    "        gmmhmmindexdict[word] = index\n",
    "        index = index +1\n",
    "\n",
    "    ############################################################################################## \n",
    "    # training GMMHMM Models \n",
    "    start = time()\n",
    "\n",
    "    speechmodels = [None] * len(spoken)\n",
    "    for key in gmmhmmindexdict:\n",
    "        speechmodels[gmmhmmindexdict[key]] = SpeechModel(gmmhmmindexdict[key],key)\n",
    "\n",
    "    for i in range(0,len(m_trainingsetfeatures)):\n",
    "         for j in range(0,len(speechmodels)):\n",
    "             if int(speechmodels[j].Class) == int(gmmhmmindexdict[m_trainingsetlabels[i]]):\n",
    "                speechmodels[j].traindata = np.concatenate((speechmodels[j].traindata , m_trainingsetfeatures[i]))\n",
    "\n",
    "    for speechmodel in speechmodels:\n",
    "        speechmodel.model.fit(speechmodel.traindata)\n",
    "\n",
    "    print ('Training completed -- {0} GMM-HMM models are built for {0} different types of words'.format(len(spoken)))\n",
    "    print('time elapsed: %.2f seconds' % ( time() - start ))\n",
    "    print (\" \"); print(\" \")\n",
    "    \n",
    "    return speechmodels, gmmhmmindexdict\n",
    "\n",
    "    '''\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction with training data started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_testingsetfeatures)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_testingsetfeatures[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_testingsetlabels)):\n",
    "        print( \"Label\"+str(i+1)+\":\"+m_testingsetlabels[i])\n",
    "        if gmmhmmindexdict[m_testingsetlabels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_testingsetlabels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing\n",
    "    ############################################################################################## \n",
    "    '''#교수님 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08525bf8-b5bc-4b28-87fa-f304681efa94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scikits.talkbox.features import mfcc\n",
    "#librosa.feature.mfcc(*, y=None, sr=22050, S=None, n_mfcc=20, dct_type=2, norm='ortho', lifter=0, **kwargs)[source]\n",
    "from librosa.feature import mfcc\n",
    "from scipy.io import wavfile\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.stats as sp\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##################################################################################\n",
    "# folder structure:\n",
    "#  ${rootpath} / ${speaker_name} / m:0-9 / ${tag}[t:0-${numtrials}]-[m:0-9]\n",
    "#    m:0-9 model number\n",
    "#    t:0-{numtrials} trial number\n",
    "#  example: train_digits('segmented-train', {'gjang', 'do', 'son'}, 'kdigis', 10) \n",
    "#           will train with\n",
    "#    segmented-train/gjang/0/kdigits0-0.wav\n",
    "#    segmented-train/gjang/0/kdigits1-0.wav\n",
    "#    ...\n",
    "#    segmented-train/son/9/kdigits8-9.wav\n",
    "#    segmented-train/son/9/kdigits9-9.wav\n",
    "##################################################################################\n",
    "def validation_digits(speechmodels, gmmhmmindexdict, rootpath, speakers, tag, num_trials=10):    \n",
    "\n",
    "    ############################################################################################## \n",
    "    # 1. find files\n",
    "    #    for user \"gjang\", digit 2, recording trial 0 (1st)\n",
    "    #    \"segmented/gjang/2/kdigits0-2.wav\"\n",
    "    # 2. extract MFCC features for training and testing\n",
    "    #    for each digit, indexes 4 and 9 for test, and the rest for training\n",
    "\n",
    "    #fpaths = []\n",
    "    #labels = []\n",
    "    spoken = []\n",
    "    m_features = []\n",
    "    m_labels = []\n",
    "\n",
    "    count = 0\n",
    "    for username in speakers:\n",
    "        apath2 = addpath(rootpath, username)    # example: segmented/gjang\n",
    "        for ii in range(10):   #dnum in os.listdir(apath2):\n",
    "            dnum = str(ii)\n",
    "            apath3 = addpath(apath2, dnum)     # example: segmented/gjang/2\n",
    "            if dnum not in spoken:\n",
    "                spoken.append(dnum)\n",
    "            for trial in range(num_trials):\n",
    "                file = addpath(apath3,\"{}{}-{}.wav\".format(tag,trial,dnum))      # segmented/gjang/2/kdigits0-2.wav\n",
    "                mc = extmfcc(file)\n",
    "\n",
    "                # display file names for the first 20 files only\n",
    "                count += 1\n",
    "                if count <= 20:\n",
    "                    print(file, dnum, end=' '); print(mc.shape, end=' ')\n",
    "                elif count == 21:\n",
    "                    print('...'); print('')\n",
    "\n",
    "                m_features.append(mc)\n",
    "                m_labels.append(dnum)\n",
    "\n",
    "    print('Words spoken:', spoken)\n",
    "    #print(\"number of labels and features = %d, %d\" % ( len(labels), len(features) ))\n",
    "    #print(\"feature shape = \", end='')\n",
    "    #print(features[0].shape)\n",
    "\n",
    "    ############################################################################################## \n",
    "    print(\"[validation] number of labels and features = %d, %d\" % ( len(m_labels), len(m_features)) )\n",
    "    print ('Loading data completed')\n",
    "\n",
    "    ############################################################################################## \n",
    "    # testing\n",
    "    print(\"Prediction started\")\n",
    "    m_PredictionlabelList = []\n",
    "\n",
    "    for i in range(0,len(m_features)):\n",
    "        scores = []\n",
    "        for speechmodel in speechmodels:\n",
    "             scores.append(speechmodel.model.score(m_features[i]))\n",
    "        id  = scores.index(max(scores))\n",
    "        m_PredictionlabelList.append(speechmodels[id].Class)\n",
    "        #print(str(np.round(scores, 3)) + \" \" + str(max(np.round(scores, 3))) +\" \"+\":\"+ speechmodels[id].label)\n",
    "\n",
    "    accuracy = 0.0\n",
    "    count = 0\n",
    "    print(\"\")\n",
    "    print(\"Prediction for Testing DataSet:\")\n",
    "\n",
    "    for i in range(0,len(m_labels)):\n",
    "        #print( \"Label\"+str(i+1)+\":\"+m_labels[i])\n",
    "        if gmmhmmindexdict[m_labels[i]] == m_PredictionlabelList[i]:\n",
    "           count = count+1\n",
    "\n",
    "    accuracy = 100.0*count/float(len(m_labels))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"accuracy =\"+str(accuracy))\n",
    "    print(\"\")\n",
    "\n",
    "    ############################################################################################## \n",
    "    # end of testing 교수님 파일\n",
    "    ############################################################################################## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d39427-c914-462d-adf7-80a5b3882ca3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/InkooJeon/0/kdigits0-0.wav 0 (255, 6) segmented-train/InkooJeon/0/kdigits1-0.wav 0 (233, 6) segmented-train/InkooJeon/0/kdigits2-0.wav 0 (265, 6) segmented-train/InkooJeon/0/kdigits3-0.wav 0 (240, 6) segmented-train/InkooJeon/0/kdigits4-0.wav 0 (249, 6) segmented-train/InkooJeon/0/kdigits5-0.wav 0 (263, 6) segmented-train/InkooJeon/0/kdigits6-0.wav 0 (272, 6) segmented-train/InkooJeon/0/kdigits7-0.wav 0 (263, 6) segmented-train/InkooJeon/0/kdigits8-0.wav 0 (254, 6) segmented-train/InkooJeon/0/kdigits9-0.wav 0 (230, 6) segmented-train/InkooJeon/1/kdigits0-1.wav 1 (250, 6) segmented-train/InkooJeon/1/kdigits1-1.wav 1 (242, 6) segmented-train/InkooJeon/1/kdigits2-1.wav 1 (248, 6) segmented-train/InkooJeon/1/kdigits3-1.wav 1 (252, 6) segmented-train/InkooJeon/1/kdigits4-1.wav 1 (228, 6) segmented-train/InkooJeon/1/kdigits5-1.wav 1 (257, 6) segmented-train/InkooJeon/1/kdigits6-1.wav 1 (252, 6) segmented-train/InkooJeon/1/kdigits7-1.wav 1 (260, 6) segmented-train/InkooJeon/1/kdigits8-1.wav 1 (249, 6) segmented-train/InkooJeon/1/kdigits9-1.wav 1 (234, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[training] number of labels and features = 600, 600\n",
      "Loading data completed\n",
      "Training completed -- 10 GMM-HMM models are built for 10 different types of words\n",
      "time elapsed: 2.42 seconds\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "speechmodels, gmmhmmindexdict = train_digits(trainroot, labels_train, 'kdigits', num_trials=10) #Train 시키는것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afc22385-4942-4995-aef2-8e3a236d73ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented-train/InkooJeon/0/kdigits0-0.wav 0 (255, 6) segmented-train/InkooJeon/0/kdigits1-0.wav 0 (233, 6) segmented-train/InkooJeon/0/kdigits2-0.wav 0 (265, 6) segmented-train/InkooJeon/0/kdigits3-0.wav 0 (240, 6) segmented-train/InkooJeon/0/kdigits4-0.wav 0 (249, 6) segmented-train/InkooJeon/0/kdigits5-0.wav 0 (263, 6) segmented-train/InkooJeon/0/kdigits6-0.wav 0 (272, 6) segmented-train/InkooJeon/0/kdigits7-0.wav 0 (263, 6) segmented-train/InkooJeon/0/kdigits8-0.wav 0 (254, 6) segmented-train/InkooJeon/0/kdigits9-0.wav 0 (230, 6) segmented-train/InkooJeon/1/kdigits0-1.wav 1 (250, 6) segmented-train/InkooJeon/1/kdigits1-1.wav 1 (242, 6) segmented-train/InkooJeon/1/kdigits2-1.wav 1 (248, 6) segmented-train/InkooJeon/1/kdigits3-1.wav 1 (252, 6) segmented-train/InkooJeon/1/kdigits4-1.wav 1 (228, 6) segmented-train/InkooJeon/1/kdigits5-1.wav 1 (257, 6) segmented-train/InkooJeon/1/kdigits6-1.wav 1 (252, 6) segmented-train/InkooJeon/1/kdigits7-1.wav 1 (260, 6) segmented-train/InkooJeon/1/kdigits8-1.wav 1 (249, 6) segmented-train/InkooJeon/1/kdigits9-1.wav 1 (234, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 600, 600\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =45.166666666666664\n",
      "\n",
      "segmented-val/org/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/org/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/org/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/org/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/org/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/org/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/org/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/org/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/org/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/org/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/org/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/org/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/org/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/org/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/org/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/org/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/org/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/org/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/org/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/org/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =25.333333333333332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_digits(speechmodels, gmmhmmindexdict, trainroot, labels_train, 'kdigits', num_trials=10)\n",
    "validation_digits(speechmodels, gmmhmmindexdict, valclean, labels_val, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cdf79c-6d34-4c91-bcf3-993188dcb566",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### noise 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e77f9be2-c131-40fc-99e9-d42b7c645212",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../audio/car.wav (175745,) [-0.01342773 -0.0222168  -0.02905273 ... -0.0390625  -0.03930664\n",
      " -0.04086304]\n",
      "../audio/car_wideband.wav (175745,) [-0.05984497 -0.14807129 -0.14700317 ... -0.10241699 -0.10253906\n",
      " -0.09594727]\n",
      "Fs = 16000, Ns = 160, Nf = 320, NFFT = 512, hNo = 257\n"
     ]
    }
   ],
   "source": [
    "audioinputpath = '../audio'\n",
    "noisefile  = addpath(audioinputpath, 'car.wav')\n",
    "wnoisefile  = addpath(audioinputpath, 'car_wideband.wav')   # 넓은 주파수 대역에 분포한 잡음\n",
    "\n",
    "Fs=16000\n",
    "noise, _ = librosa.load(noisefile, sr=Fs, mono=True)\n",
    "wnoise, _ = librosa.load(wnoisefile, sr=Fs, mono=True)\n",
    "# sr: target sampling rate. ‘None’ uses the native sampling rate\n",
    "# mono = True: convert signal to mono\n",
    "\n",
    "print(noisefile, noise.shape, noise)\n",
    "print(wnoisefile, wnoise.shape, wnoise)\n",
    "\n",
    "Ns = int(Fs*Ts)    # shift number of samples\n",
    "Nf = int(Fs*Tf)    # frame number of samples\n",
    "NFFT = int(2**(np.ceil(np.log2(Nf))))   # Nf보다 크거나 같은 2의 거듭제곱을 NFFT 로 정의\n",
    "hNo = NFFT//2+1\n",
    "print('Fs = %d, Ns = %d, Nf = %d, NFFT = %d, hNo = %d' % (Fs, Ns, Nf, NFFT, hNo)) #audio/car 파일 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48972-47cc-4af7-8de7-96a8dd40676f",
   "metadata": {},
   "source": [
    "__generate noisy speech with various SNRs__\n",
    "- 음성과 잡음의 상대적 크기에 따라 잡음의 효과를 time domain, spectrogram, 그리고 들어서 확인해 본다.\n",
    "- mixed input $x[t]$ 를 다음과 같이 생성한다.\n",
    "$$ x[t] = s[t] + 10^{-r/20} \\frac{\\sigma_{s}}{\\sigma_{n}} n[t] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620a2571-2da9-4505-b211-2cfa13816693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_mixed_signals_2(speech, noise, SNRs, isdraw=False):\n",
    "    std_s = np.sqrt(np.mean(speech**2))\n",
    "    std_n = np.sqrt(np.mean(noise[:len(speech)]**2))\n",
    "    mixedSig = []\n",
    "    for snr in SNRs:\n",
    "        gain = np.power(10, -snr/20)\n",
    "        gn = noise[:len(speech)]/std_n*std_s*gain\n",
    "        m = speech + gn\n",
    "        mixedSig.append(m)\n",
    "\n",
    "    return mixedSig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "754fc25f-425b-4277-885b-411d5d65f067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audioroot = valroot\n",
    "audioclean = valclean\n",
    "labels = labels_val\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')] \n",
    "SNRs = [10, 0, -10] \n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            infile = addpath(audioclean, addpath(subname, basename))            \n",
    "            num_files += 1\n",
    "            \n",
    "            signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "            nbnsig = generate_mixed_signals_2(signal, noise, SNRs, False)\n",
    "            wbnsig = generate_mixed_signals_2(signal, wnoise, SNRs, False)\n",
    "            noisy = [nbnsig, wbnsig] #노이즈 넣는 부분\n",
    "            \n",
    "            for jj in range(len(noisy)):\n",
    "                for n in range(len(noisy[jj])):\n",
    "                    outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                    wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0) #저장\n",
    "\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n])) #노이즈 추가된 파일 경로 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54809aa7-63c6-4e11-b42e-8f59ef3495df",
   "metadata": {},
   "source": [
    "Noise model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46abea0e-484f-4dd2-ba19-d66a44178ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "testing segmented-val/nbnSNR10\n",
      "segmented-val/nbnSNR10/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/nbnSNR10/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/nbnSNR10/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/nbnSNR10/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/nbnSNR10/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/nbnSNR10/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/nbnSNR10/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/nbnSNR10/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/nbnSNR10/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/nbnSNR10/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/nbnSNR10/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/nbnSNR10/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/nbnSNR10/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/nbnSNR10/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/nbnSNR10/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/nbnSNR10/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/nbnSNR10/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR10/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/nbnSNR10/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/nbnSNR10/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =26.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR0\n",
      "segmented-val/nbnSNR0/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/nbnSNR0/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/nbnSNR0/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/nbnSNR0/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/nbnSNR0/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/nbnSNR0/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/nbnSNR0/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/nbnSNR0/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/nbnSNR0/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/nbnSNR0/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/nbnSNR0/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/nbnSNR0/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/nbnSNR0/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/nbnSNR0/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/nbnSNR0/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/nbnSNR0/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/nbnSNR0/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR0/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/nbnSNR0/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/nbnSNR0/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =18.666666666666668\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/nbnSNR-10\n",
      "segmented-val/nbnSNR-10/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/nbnSNR-10/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/nbnSNR-10/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/nbnSNR-10/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/nbnSNR-10/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/nbnSNR-10/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/nbnSNR-10/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/nbnSNR-10/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/nbnSNR-10/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/nbnSNR-10/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/nbnSNR-10/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/nbnSNR-10/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/nbnSNR-10/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/nbnSNR-10/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/nbnSNR-10/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/nbnSNR-10/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/nbnSNR-10/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/nbnSNR-10/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/nbnSNR-10/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/nbnSNR-10/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =11.333333333333334\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR10\n",
      "segmented-val/wbnSNR10/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/wbnSNR10/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/wbnSNR10/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/wbnSNR10/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/wbnSNR10/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/wbnSNR10/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/wbnSNR10/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/wbnSNR10/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/wbnSNR10/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/wbnSNR10/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/wbnSNR10/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/wbnSNR10/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/wbnSNR10/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/wbnSNR10/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/wbnSNR10/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/wbnSNR10/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/wbnSNR10/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR10/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/wbnSNR10/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/wbnSNR10/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =20.333333333333332\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR0\n",
      "segmented-val/wbnSNR0/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/wbnSNR0/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/wbnSNR0/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/wbnSNR0/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/wbnSNR0/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/wbnSNR0/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/wbnSNR0/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/wbnSNR0/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/wbnSNR0/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/wbnSNR0/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/wbnSNR0/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/wbnSNR0/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/wbnSNR0/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/wbnSNR0/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/wbnSNR0/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/wbnSNR0/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/wbnSNR0/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR0/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/wbnSNR0/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/wbnSNR0/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "--------------------------------\n",
      "testing segmented-val/wbnSNR-10\n",
      "segmented-val/wbnSNR-10/chlee/0/kdigits0-0.wav 0 (98, 6) segmented-val/wbnSNR-10/chlee/0/kdigits1-0.wav 0 (105, 6) segmented-val/wbnSNR-10/chlee/0/kdigits2-0.wav 0 (94, 6) segmented-val/wbnSNR-10/chlee/0/kdigits3-0.wav 0 (96, 6) segmented-val/wbnSNR-10/chlee/0/kdigits4-0.wav 0 (100, 6) segmented-val/wbnSNR-10/chlee/0/kdigits5-0.wav 0 (95, 6) segmented-val/wbnSNR-10/chlee/0/kdigits6-0.wav 0 (96, 6) segmented-val/wbnSNR-10/chlee/0/kdigits7-0.wav 0 (90, 6) segmented-val/wbnSNR-10/chlee/0/kdigits8-0.wav 0 (108, 6) segmented-val/wbnSNR-10/chlee/0/kdigits9-0.wav 0 (95, 6) segmented-val/wbnSNR-10/chlee/1/kdigits0-1.wav 1 (121, 6) segmented-val/wbnSNR-10/chlee/1/kdigits1-1.wav 1 (103, 6) segmented-val/wbnSNR-10/chlee/1/kdigits2-1.wav 1 (92, 6) segmented-val/wbnSNR-10/chlee/1/kdigits3-1.wav 1 (101, 6) segmented-val/wbnSNR-10/chlee/1/kdigits4-1.wav 1 (108, 6) segmented-val/wbnSNR-10/chlee/1/kdigits5-1.wav 1 (95, 6) segmented-val/wbnSNR-10/chlee/1/kdigits6-1.wav 1 (96, 6) segmented-val/wbnSNR-10/chlee/1/kdigits7-1.wav 1 (97, 6) segmented-val/wbnSNR-10/chlee/1/kdigits8-1.wav 1 (104, 6) segmented-val/wbnSNR-10/chlee/1/kdigits9-1.wav 1 (91, 6) ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 300, 300\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =7.333333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in outputpaths:\n",
    "    print('--------------------------------')\n",
    "    print('testing', path)\n",
    "    validation_digits(speechmodels, gmmhmmindexdict, path, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5330b-3c8a-4170-aedf-1376dceec3cf",
   "metadata": {},
   "source": [
    "EPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "468a39e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "audioroot = 'unsegmented-test'\n",
    "audioclean = addpath(audioroot,'org')\n",
    "labels = ['gjang']\n",
    "tag='kdigits'\n",
    "num_trials = 10\n",
    "apath = addpath(audioclean, labels[0])    # ex: unsegmented-test/org/gjang 경로를 지정\n",
    "apath2 = []\n",
    "# WAV파일 열기\n",
    "for i in range(num_trials): \n",
    "    apath2.append(addpath(apath, str(i)))   # ex: unsegmented-test/org/gjang/0 - 9까지 리스트 만들기\n",
    "    if not os.path.exists(apath2[i]):\n",
    "        os.makedirs(apath2[i])\n",
    "for j in range(10): \n",
    "    file = addpath(apath,\"{}{}.wav\".format(tag,j)) # ex: unsegmented-test/org/gjang/kdigits0.wav\n",
    "    # pydub을 사용한 오디오파일 불러오기\n",
    "    audio = AudioSegment.from_wav(file)\n",
    "    # 오디오 파일 추출\n",
    "    nonsilent = detect_nonsilent(audio, min_silence_len=500, silence_thresh=-40)\n",
    "    # 오디오 추출 결과(음성신호 시작시간, 끝시간)\n",
    "    time = []\n",
    "    for i, part in enumerate(nonsilent):\n",
    "        time.append([part[0]-500,part[1]+500]) # 앞뒤 0.5초 딜레이 추가\n",
    "    for i in range(10): #unsegmented-test\\org\\gjang/0~9에 저장\n",
    "        output_file = addpath(apath2[i],\"kdigits{}-{}.wav\".format(j,i)) \n",
    "        if i==0:\n",
    "            interval_audio = audio[time[num_trials-1][0]:time[num_trials-1][1]]\n",
    "            interval_audio.export(output_file, format=\"wav\")\n",
    "        else:\n",
    "            interval_audio = audio[time[i-1][0]:time[i-1][1]]\n",
    "            interval_audio.export(output_file, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7340c08d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsegmented-test/org/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/org/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/org/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/org/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/org/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/org/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/org/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/org/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/org/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/org/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/org/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/org/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/org/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/org/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/org/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/org/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/org/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/org/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/org/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/org/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =12.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 정확도 측정\n",
    "validation_digits(speechmodels, gmmhmmindexdict, audioclean, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31a079a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##오디오 파일에 노이즈 파일 추가.\n",
    "##오지현 학생의 코드를 참고하였음.\n",
    "\n",
    "noisyroots = [addpath(audioroot,'nbnSNR'), addpath(audioroot,'wbnSNR')] # ['unsegmented-test/nbnSNR', 'unsegmented-test/wbnSNR']\n",
    "SNRs = [10, 0, -10]\n",
    "\n",
    "num_trials = 10\n",
    "noisy = [nbnsig, wbnsig]\n",
    "cnt = 0\n",
    "path = []\n",
    "path2 = []\n",
    "\n",
    "# Open new WAV file for writing\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        path.append(addpath('%s%d'%(noisyroots[jj],SNRs[n]), '%s'%(labels[0])))\n",
    "        for trial in range(num_trials):\n",
    "            path2.append(addpath(path[n+3*jj], str(trial)))   # ex: unsegmented-test/nbnSNR10/gjang/0\n",
    "            if not os.path.exists(path2[trial+10*n+30*jj]):   \n",
    "                os.makedirs(path2[trial+10*n+30*jj])\n",
    "\n",
    "for subname in labels:\n",
    "    num_files = 0\n",
    "    for w in range(10):\n",
    "        for trial in range(10):\n",
    "            basename = '%d/kdigits%d-%d.wav'%(w,trial,w)\n",
    "            infile = addpath(audioclean, addpath(subname, basename))\n",
    "            num_files += 1\n",
    "            \n",
    "            signal, Fs = librosa.load(infile, sr=Fs, mono=True)\n",
    "            nbnsig = generate_mixed_signals_2(signal, np.concatenate((noise,noise,noise)), SNRs, False)\n",
    "            wbnsig = generate_mixed_signals_2(signal, np.concatenate((wnoise,wnoise,wnoise)), SNRs, False)\n",
    "            noisy = [nbnsig, wbnsig]\n",
    "\n",
    "            for jj in range(len(noisy)):\n",
    "                for n in range(len(noisy[jj])):\n",
    "                    outfile = addpath('%s%d'%(noisyroots[jj],SNRs[n]), addpath(subname, basename))\n",
    "                    wav.writewav(outfile, Fs, noisy[jj][n], maxval=1.0)\n",
    "outputpaths = []\n",
    "for jj in range(len(noisy)):\n",
    "    for n in range(len(noisy[jj])):\n",
    "        outputpaths.append('%s%d'%(noisyroots[jj],SNRs[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bfc336",
   "metadata": {},
   "source": [
    "HMM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2a0f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/nbnSNR10/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/nbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/nbnSNR10/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =21.0\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/nbnSNR0/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/nbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/nbnSNR0/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =20.0\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/nbnSNR-10/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/nbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/nbnSNR-10/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/wbnSNR10/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/wbnSNR10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/wbnSNR10/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =35.0\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/wbnSNR0/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/wbnSNR0/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/wbnSNR0/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n",
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =19.0\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "unsegmented-test/wbnSNR-10/gjang/0/kdigits0-0.wav 0 (124, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits1-0.wav 0 (123, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits2-0.wav 0 (122, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits3-0.wav 0 (122, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits4-0.wav 0 (121, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits5-0.wav 0 (122, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits6-0.wav 0 (122, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits7-0.wav 0 (122, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits8-0.wav 0 (123, 6) unsegmented-test/wbnSNR-10/gjang/0/kdigits9-0.wav 0 (124, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits0-1.wav 1 (129, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits1-1.wav 1 (131, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits2-1.wav 1 (129, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits3-1.wav 1 (125, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits4-1.wav 1 (124, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits5-1.wav 1 (125, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits6-1.wav 1 (127, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits7-1.wav 1 (127, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits8-1.wav 1 (128, 6) unsegmented-test/wbnSNR-10/gjang/1/kdigits9-1.wav 1 (130, 6) ...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words spoken: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "[validation] number of labels and features = 100, 100\n",
      "Loading data completed\n",
      "Prediction started\n",
      "\n",
      "Prediction for Testing DataSet:\n",
      "\n",
      "accuracy =10.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for outpath in outputpaths:\n",
    "    print('---------------------------------------------------------------------------')\n",
    "    validation_digits(speechmodels, gmmhmmindexdict, outpath, labels, 'kdigits', num_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ef5f6",
   "metadata": {},
   "source": [
    "Wiener filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12b09b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import wiener\n",
    "path_filter = addpath(audioroot, 'Wiener')\n",
    "if not os.path.exists(path_filter):\n",
    "    os.makedirs(path_filter)\n",
    "for i in range(3):\n",
    "        speech_noise, Fs_noise = librosa.load(addpath(path2[i*10+3], 'kdigits5-3.wav'), sr=None, mono=True)\n",
    "        filtered_data = wiener(speech_noise)\n",
    "        outfile = addpath(path_filter, \"{}{}-{}_{}{}.wav\".format(i,0,3,'nbnSNR',SNRs[i]))\n",
    "        wav.writewav(outfile, Fs_noise, filtered_data)\n",
    "        speech_noise, Fs_noise = librosa.load(addpath(path2[i*10+3], 'kdigits5-3.wav'), sr=None, mono=True)\n",
    "        filtered_data = wiener(speech_noise)\n",
    "        outfile = addpath(path_filter, \"{}{}-{}_{}{}.wav\".format(i,0,3,'wbnSNR',SNRs[i-3]))\n",
    "        wav.writewav(outfile, Fs_noise, filtered_data)             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615b35b-48f6-4617-9c55-0ee6a5f1ecdf",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8db264-367a-4400-9f13-86dc7347ce9a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2cc10f-ae74-4d39-bcdb-5863f58cb805",
   "metadata": {},
   "source": [
    "## End of Project 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
